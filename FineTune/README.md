![robot](imgs/head1.png)

# å¾®è°ƒæ•°æ®æ„é€ å®éªŒ

## 1 xtuneræ•™å­¦
è¯¦è§ï¼š
## 2 å¾®è°ƒå®æˆ˜

> è¿™é‡Œé‡‡ç”¨ **[COIG-CQIA](https://opendatalab.org.cn/OpenDataLab/COIG-CQIA)** **æ•°æ®é›†**ä¸­çš„å¼±æ™ºå§æ•°æ®

### 2.1 æ¦‚è¿°

#### 2.1.1 **åœºæ™¯éœ€æ±‚**

   åŸºäº InternLM-chat-7B æ¨¡å‹ï¼Œç”¨å¼±æ™ºå§ä¸­çš„æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œè®­ç»ƒä¸€ä¸ªä¸å¼±æ™ºçš„æ¨¡å‹

#### 2.1.2 **çœŸå®æ•°æ®é¢„è§ˆ**

| é—®é¢˜                                                       | ç­”æ¡ˆ                                                         |
| ---------------------------------------------------------- | ------------------------------------------------------------ |
|     | |


### 2.2 æ•°æ®å‡†å¤‡ 

#### 2.2.1 æ•°æ®é›†ä¸‹è½½

**åŸæ ¼å¼ï¼š(.jsonl)**

#### 2.2.2 å°†æ•°æ®è½¬ä¸º XTuner çš„æ•°æ®æ ¼å¼

**ç›®æ ‡æ ¼å¼ï¼š(.jsonL)**

```JSON
[{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
},
{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
}]
```

ğŸ§ é‡‡ç”¨GPTçš„data analysisåŠŸèƒ½å¯ç›´æ¥å®Œæˆæ•°æ®çš„è½¬æ¢

> è¿™ä¸€æ­¥çš„ python è„šæœ¬å¯ä»¥è¯· ChatGPT æ¥å®Œæˆã€‚

```text
Write a python file for me. using openpyxl. input file name is MedQA2019.xlsx
Step1: The input file is .xlsx. Exact the column A and column D in the sheet named "DrugQA" .
Step2: Put each value in column A into each "input" of each "conversation". Put each value in column D into each "output" of each "conversation".
Step3: The output file is .jsonL. It looks like:
[{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
},
{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
}]
Step4: All "system" value changes to "You are a professional, highly experienced doctor professor. You always provide accurate, comprehensive, and detailed answers based on the patients' questions."
```

> ChatGPT ç”Ÿæˆçš„ python ä»£ç è§æœ¬ä»“åº“çš„ [xlsx2jsonl.py](./xlsx2jsonl.py)


æ‰§è¡Œ python è„šæœ¬ï¼Œè·å¾—æ ¼å¼åŒ–åçš„æ•°æ®é›†ï¼š
```bash
python xlsx2jsonl.py
```

**æ ¼å¼åŒ–åçš„æ•°æ®é›†é•¿è¿™æ ·ï¼š**
![uOCJXwbfzKRWSBE.png](imgs/dataProcessed.png)

æ­¤æ—¶ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥å¯¹æ•°æ®è¿›è¡Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ†å‰²ï¼ŒåŒæ ·å¯ä»¥è®© ChatGPT å†™ python ä»£ç ã€‚å½“ç„¶å¦‚æœä½ æ²¡æœ‰ä¸¥æ ¼çš„ç§‘ç ”éœ€æ±‚ã€ä¸åœ¨ä¹â€œè®­ç»ƒé›†æ³„éœ²â€çš„é—®é¢˜ï¼Œä¹Ÿå¯ä»¥ä¸åšè®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„åˆ†å‰²ã€‚

#### 2.2.3 åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†

```text
my .jsonL file looks like:
[{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
},
{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
}]
Step1, read the .jsonL file.
Step2, count the amount of the "conversation" elements.
Step3, randomly split all "conversation" elements by 7:3. Targeted structure is same as the input.
Step4, save the 7/10 part as train.jsonl. save the 3/10 part as test.jsonl
```
ç”Ÿæˆçš„pythonä»£ç è§ [split2train_and_test.py](./split2train_and_test.py)


### 2.3 å¼€å§‹è‡ªå®šä¹‰å¾®è°ƒ

æ­¤æ—¶ï¼Œæˆ‘ä»¬é‡æ–°å»ºä¸€ä¸ªæ–‡ä»¶å¤¹æ¥ç©â€œå¾®è°ƒè‡ªå®šä¹‰æ•°æ®é›†â€
```bash
mkdir ~/ft-medqa && cd ~/ft-medqa
```

æŠŠå‰é¢ä¸‹è½½å¥½çš„internlm-chat-7bæ¨¡å‹æ–‡ä»¶å¤¹æ‹·è´è¿‡æ¥ã€‚

```bash
cp -r ~/ft-oasst1/internlm-chat-7b .
```
åˆ«å¿˜äº†æŠŠè‡ªå®šä¹‰æ•°æ®é›†ï¼Œå³å‡ ä¸ª `.jsonL`ï¼Œä¹Ÿä¼ åˆ°æœåŠ¡å™¨ä¸Šã€‚

```bash
git clone https://github.com/InternLM/tutorial
```

```bash
cp ~/tutorial/xtuner/MedQA2019-structured-train.jsonl .
```



#### 2.3.1 å‡†å¤‡é…ç½®æ–‡ä»¶
```bash
# å¤åˆ¶é…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•
xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .
# æ”¹ä¸ªæ–‡ä»¶å
mv internlm_chat_7b_qlora_oasst1_e3_copy.py internlm_chat_7b_qlora_medqa2019_e3.py

# ä¿®æ”¹é…ç½®æ–‡ä»¶å†…å®¹
vim internlm_chat_7b_qlora_medqa2019_e3.py
```

å‡å·ä»£è¡¨è¦åˆ é™¤çš„è¡Œï¼ŒåŠ å·ä»£è¡¨è¦å¢åŠ çš„è¡Œã€‚
```diff
# ä¿®æ”¹importéƒ¨åˆ†
- from xtuner.dataset.map_fns import oasst1_map_fn, template_map_fn_factory
+ from xtuner.dataset.map_fns import template_map_fn_factory

# ä¿®æ”¹æ¨¡å‹ä¸ºæœ¬åœ°è·¯å¾„
- pretrained_model_name_or_path = 'internlm/internlm-chat-7b'
+ pretrained_model_name_or_path = './internlm-chat-7b'

# ä¿®æ”¹è®­ç»ƒæ•°æ®ä¸º MedQA2019-structured-train.jsonl è·¯å¾„
- data_path = 'timdettmers/openassistant-guanaco'
+ data_path = 'MedQA2019-structured-train.jsonl'

# ä¿®æ”¹ train_dataset å¯¹è±¡
train_dataset = dict(
    type=process_hf_dataset,
-   dataset=dict(type=load_dataset, path=data_path),
+   dataset=dict(type=load_dataset, path='json', data_files=dict(train=data_path)),
    tokenizer=tokenizer,
    max_length=max_length,
-   dataset_map_fn=alpaca_map_fn,
+   dataset_map_fn=None,
    template_map_fn=dict(
        type=template_map_fn_factory, template=prompt_template),
    remove_unused_columns=True,
    shuffle_before_pack=True,
    pack_to_max_length=pack_to_max_length)
```
#### 2.3.2 **XTunerï¼å¯åŠ¨ï¼**

![tH8udZzECYl5are.png](imgs/ysqd.png)

```bash
xtuner train internlm_chat_7b_qlora_medqa2019_e3.py --deepspeed deepspeed_zero2
```

#### 2.3.3 pth è½¬ huggingface

å°†å¾—åˆ°çš„ PTH æ¨¡å‹è½¬æ¢ä¸º HuggingFace æ¨¡å‹ï¼Œ**å³ï¼šç”Ÿæˆ Adapter æ–‡ä»¶å¤¹**

`xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH_file_dir} ${SAVE_PATH}`

åœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œä¸ºï¼š
```bash
mkdir hf
export MKL_SERVICE_FORCE_INTEL=1
export MKL_THREADING_LAYER=GNU
xtuner convert pth_to_hf ./internlm_chat_7b_qlora_oasst1_e3_copy.py ./work_dirs/internlm_chat_7b_qlora_oasst1_e3_copy/epoch_1.pth ./hf
```
æ­¤æ—¶ï¼Œè·¯å¾„ä¸­åº”è¯¥é•¿è¿™æ ·ï¼š

```Bash
|-- internlm-chat-7b
|-- internlm_chat_7b_qlora_oasst1_e3_copy.py
|-- openassistant-guanaco
|   |-- openassistant_best_replies_eval.jsonl
|   `-- openassistant_best_replies_train.jsonl
|-- hf
|   |-- README.md
|   |-- adapter_config.json
|   |-- adapter_model.bin
|   `-- xtuner_config.py
`-- work_dirs
    `-- internlm_chat_7b_qlora_oasst1_e3_copy
        |-- 20231101_152923
        |   |-- 20231101_152923.log
        |   `-- vis_data
        |       |-- 20231101_152923.json
        |       |-- config.py
        |       `-- scalars.json
        |-- epoch_1.pth
        |-- epoch_2.pth
        |-- epoch_3.pth
        |-- internlm_chat_7b_qlora_oasst1_e3_copy.py
        `-- last_checkpoint
```

<span style="color: red;">**æ­¤æ—¶ï¼Œhf æ–‡ä»¶å¤¹å³ä¸ºæˆ‘ä»¬å¹³æ—¶æ‰€ç†è§£çš„æ‰€è°“ â€œLoRA æ¨¡å‹æ–‡ä»¶â€**</span>

> å¯ä»¥ç®€å•ç†è§£ï¼šLoRA æ¨¡å‹æ–‡ä»¶ = Adapter

#### 2.4 éƒ¨ç½²ä¸æµ‹è¯•

#### 2.4.1 å°† HuggingFace adapter åˆå¹¶åˆ°å¤§è¯­è¨€æ¨¡å‹ï¼š

```Bash
xtuner convert merge ./internlm-chat-7b ./hf ./merged --max-shard-size 2GB
# xtuner convert merge \
#     ${NAME_OR_PATH_TO_LLM} \
#     ${NAME_OR_PATH_TO_ADAPTER} \
#     ${SAVE_PATH} \
#     --max-shard-size 2GB
```

#### 2.4.2 ä¸åˆå¹¶åçš„æ¨¡å‹å¯¹è¯ï¼š
```Bash
# åŠ è½½ Adapter æ¨¡å‹å¯¹è¯ï¼ˆFloat 16ï¼‰
xtuner chat ./merged --prompt-template internlm_chat

# 4 bit é‡åŒ–åŠ è½½
# xtuner chat ./merged --bits 4 --prompt-template internlm_chat
```

#### 2.4.3 Demo

- ä¿®æ”¹ `cli_demo.py` ä¸­çš„æ¨¡å‹è·¯å¾„
```diff
- model_name_or_path = "/root/model/Shanghai_AI_Laboratory/internlm-chat-7b"
+ model_name_or_path = "merged"
```
- è¿è¡Œ `cli_demo.py` ä»¥ç›®æµ‹å¾®è°ƒæ•ˆæœ
```bash
python ./cli_demo.py
```

**æ•ˆæœï¼š**

| å¾®è°ƒå‰ | å¾®è°ƒå |
| --- | --- |
| ![O23QD48iFSZMfbr.png](imgs/beforeFT.png) | ![L1sqmGgE6h2exWP.png](imgs/afterFT.png) |

**`xtuner chat`** **çš„å¯åŠ¨å‚æ•°**

| å¯åŠ¨å‚æ•°              | å¹²å“ˆæ»´                                                       |
| --------------------- | ------------------------------------------------------------ |
| **--prompt-template** | æŒ‡å®šå¯¹è¯æ¨¡æ¿                                                 |
| --system              | æŒ‡å®šSYSTEMæ–‡æœ¬                                               |
| --system-template     | æŒ‡å®šSYSTEMæ¨¡æ¿                                               |
| -**-bits**            | LLMä½æ•°                                                      |
| --bot-name            | botåç§°                                                      |
| --with-plugins        | æŒ‡å®šè¦ä½¿ç”¨çš„æ’ä»¶                                             |
| **--no-streamer**     | æ˜¯å¦å¯ç”¨æµå¼ä¼ è¾“                                             |
| **--lagent**          | æ˜¯å¦ä½¿ç”¨lagent                                               |
| --command-stop-word   | å‘½ä»¤åœæ­¢è¯                                                   |
| --answer-stop-word    | å›ç­”åœæ­¢è¯                                                   |
| --offload-folder      | å­˜æ”¾æ¨¡å‹æƒé‡çš„æ–‡ä»¶å¤¹ï¼ˆæˆ–è€…å·²ç»å¸è½½æ¨¡å‹æƒé‡çš„æ–‡ä»¶å¤¹ï¼‰         |
| --max-new-tokens      | ç”Ÿæˆæ–‡æœ¬ä¸­å…è®¸çš„æœ€å¤§ `token` æ•°é‡                                |
| **--temperature**     | æ¸©åº¦å€¼                                                       |
| --top-k               | ä¿ç•™ç”¨äºé¡¶kç­›é€‰çš„æœ€é«˜æ¦‚ç‡è¯æ±‡æ ‡è®°æ•°                          |
| --top-p               | å¦‚æœè®¾ç½®ä¸ºå°äº1çš„æµ®ç‚¹æ•°ï¼Œä»…ä¿ç•™æ¦‚ç‡ç›¸åŠ é«˜äº `top_p` çš„æœ€å°ä¸€ç»„æœ€æœ‰å¯èƒ½çš„æ ‡è®° |
| --seed                | ç”¨äºå¯é‡ç°æ–‡æœ¬ç”Ÿæˆçš„éšæœºç§å­                                 |

åŒå‰è¿°ã€‚[éƒ¨ç½²ä¸æµ‹è¯•](#24-éƒ¨ç½²ä¸æµ‹è¯•)



## 3 å…¶ä»–å·²çŸ¥é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼š
https://docs.qq.com/doc/DY1d2ZVFlbXlrUERj


å°ä½œä¸šåŠ©æ•™è€å¸ˆä¼šåœ¨ç¤¾ç¾¤ä¸­å…¬å¸ƒã€‚
Have fun!



## 4 æ³¨æ„äº‹é¡¹

æœ¬æ•™ç¨‹ä½¿ç”¨ xtuner 0.1.9 ç‰ˆæœ¬
è‹¥éœ€è¦è·Ÿç€æœ¬æ•™ç¨‹ä¸€æ­¥ä¸€æ­¥å®Œæˆï¼Œå»ºè®®ä¸¥æ ¼éµå¾ªæœ¬æ•™ç¨‹çš„æ­¥éª¤ï¼



è‹¥å‡ºç°è«åå…¶å¦™æŠ¥é”™ï¼Œè¯·å°è¯•æ›´æ¢ä¸ºä»¥ä¸‹åŒ…çš„ç‰ˆæœ¬ï¼šï¼ˆå¦‚æœæœ‰æŠ¥é”™å†æ£€æŸ¥ï¼Œæ²¡æŠ¥é”™ä¸ç”¨çœ‹ï¼‰
```
torch                         2.1.1
transformers                  4.34.0
transformers-stream-generator 0.0.4
```
```bash
pip install torch==2.1.1
pip install transformers==4.34.0
pip install transformers-stream-generator=0.0.4
```
CUDA ç›¸å…³ï¼šï¼ˆå¦‚æœæœ‰æŠ¥é”™å†æ£€æŸ¥ï¼Œæ²¡æŠ¥é”™ä¸ç”¨çœ‹ï¼‰
```
NVIDIA-SMI 535.54.03              
Driver Version: 535.54.03    
CUDA Version: 12.2

nvidia-cuda-cupti-cu12        12.1.105
nvidia-cuda-nvrtc-cu12        12.1.105
nvidia-cuda-runtime-cu12      12.1.105
```

## 5 ä½œä¸š

**åŸºç¡€ä½œä¸šï¼š**

æ„å»ºæ•°æ®é›†ï¼Œä½¿ç”¨ XTuner å¾®è°ƒ InternLM-Chat-7B æ¨¡å‹, è®©æ¨¡å‹å­¦ä¹ åˆ°å®ƒæ˜¯ä½ çš„æ™ºèƒ½å°åŠ©æ‰‹ï¼Œæ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæœ¬ä½œä¸šè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹çš„è¾“å‡ºéœ€è¦**å°†ä¸è¦è‘±å§œè’œå¤§ä½¬**æ›¿æ¢æˆè‡ªå·±åå­—æˆ–æ˜µç§°ï¼

**å¾®è°ƒå‰**ï¼ˆå›ç­”æ¯”è¾ƒå®˜æ–¹ï¼‰
![web_show_2.png](imgs%2Fweb_show_2.png)


**å¾®è°ƒå**ï¼ˆå¯¹è‡ªå·±çš„èº«ä»½æœ‰äº†æ¸…æ™°çš„è®¤çŸ¥ï¼‰
![web_show_1.png](imgs%2Fweb_show_1.png)

ä½œä¸šå‚è€ƒç­”æ¡ˆï¼šhttps://github.com/InternLM/tutorial/blob/main/xtuner/self.md

**è¿›é˜¶ä½œä¸šï¼š**

- å°†è®­ç»ƒå¥½çš„Adapteræ¨¡å‹æƒé‡ä¸Šä¼ åˆ° OpenXLabã€Hugging Face æˆ–è€… MoelScope ä»»ä¸€ä¸€å¹³å°ã€‚
- å°†è®­ç»ƒå¥½åçš„æ¨¡å‹åº”ç”¨éƒ¨ç½²åˆ° OpenXLab å¹³å°ï¼Œå‚è€ƒéƒ¨ç½²æ–‡æ¡£è¯·è®¿é—®ï¼šhttps://aicarrier.feishu.cn/docx/MQH6dygcKolG37x0ekcc4oZhnCe

**æ•´ä½“å®è®­è¥é¡¹ç›®ï¼š**

æ—¶é—´å‘¨æœŸï¼šå³æ—¥èµ·è‡´è¯¾ç¨‹ç»“æŸ

å³æ—¥å¼€å§‹å¯ä»¥åœ¨ç­çº§ç¾¤ä¸­éšæœºç»„é˜Ÿå®Œæˆä¸€ä¸ªå¤§ä½œä¸šé¡¹ç›®ï¼Œä¸€äº›å¯æä¾›çš„é€‰é¢˜å¦‚ä¸‹ï¼š

- äººæƒ…ä¸–æ•…å¤§æ¨¡å‹ï¼šä¸€ä¸ªå¸®åŠ©ç”¨æˆ·æ’°å†™æ–°å¹´ç¥ç¦æ–‡æ¡ˆçš„äººæƒ…äº‹æ•…å¤§æ¨¡å‹
- ä¸­å°å­¦æ•°å­¦å¤§æ¨¡å‹ï¼šä¸€ä¸ªæ‹¥æœ‰ä¸€å®šæ•°å­¦è§£é¢˜èƒ½åŠ›çš„å¤§æ¨¡å‹
- å¿ƒç†å¤§æ¨¡å‹ï¼šä¸€ä¸ªæ²»æ„ˆçš„å¿ƒç†å¤§æ¨¡å‹
- å·¥å…·è°ƒç”¨ç±»é¡¹ç›®ï¼šç»“åˆ Lagent æ„å»ºæ•°æ®é›†è®­ç»ƒ InternLM æ¨¡å‹ï¼Œæ”¯æŒå¯¹ MMYOLO ç­‰å·¥å…·çš„è°ƒç”¨

å…¶ä»–åŸºäºä¹¦ç”ŸÂ·æµ¦è¯­å·¥å…·é“¾çš„å°é¡¹ç›®éƒ½åœ¨èŒƒå›´å†…ï¼Œæ¬¢è¿å¤§å®¶å……åˆ†å‘æŒ¥æƒ³è±¡åŠ›ã€‚

